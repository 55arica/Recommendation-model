{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2aam4xKYr2-v"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Activation\n",
        "from tensorflow.keras.optimizers import RMSprop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Activation\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "\n",
        "dataset = pd.read_csv(\"Book1.csv\")\n",
        "\n",
        "\n",
        "data = list(dataset['Paragraph'].values)\n",
        "\n",
        "\n",
        "text = \"\".join(data)\n",
        "\n",
        "\n",
        "text_length = text[:10000]\n",
        "\n",
        "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
        "tokens = tokenizer.tokenize(text_length.lower())\n",
        "\n",
        "unique_tokens = np.unique(tokens)\n",
        "unique_tokens_index = {token: idx for idx, token in enumerate(unique_tokens)}\n",
        "\n",
        "n_words = 10\n",
        "input_word = []\n",
        "next_word = []\n",
        "\n",
        "for i in range(len(tokens) - n_words):\n",
        "    input_word.append(tokens[i:i + n_words])\n",
        "    next_word.append(tokens[i + n_words])\n",
        "\n",
        "x = np.zeros((len(input_word), n_words, len(unique_tokens)), dtype=bool)\n",
        "y = np.zeros((len(next_word), len(unique_tokens)), dtype=bool)\n",
        "\n",
        "for i, words in enumerate(input_word):\n",
        "    for j, word in enumerate(words):\n",
        "        x[i, j, unique_tokens_index[word]] = 1\n",
        "    y[i, unique_tokens_index[next_word[i]]] = 1\n",
        "\n",
        "# Define and compile the model\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(n_words, len(unique_tokens)), return_sequences=True))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(len(unique_tokens)))\n",
        "model.add(Activation(\"softmax\"))\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=RMSprop(learning_rate=0.01), metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit the model\n",
        "model.fit(x, y, batch_size=128, epochs=30, shuffle=True)\n",
        "\n",
        "def predict_next_word(input_text, n_best):\n",
        "    input_text = input_text.lower()\n",
        "    x = np.zeros((1, n_words, len(unique_tokens)))\n",
        "    for i, word in enumerate(input_text.split()):\n",
        "        x[0, i, unique_tokens_index[word]] = 1\n",
        "\n",
        "    predictions = model.predict(x)[0]\n",
        "    return np.argpartition(predictions, -n_best)[-n_best:]\n",
        "\n",
        "# Define the generate_text function\n",
        "def generate_text(input_text, text_length, creativity=3):\n",
        "    word_sequence = input_text.split()\n",
        "    current = 0\n",
        "    word_tokens = []\n",
        "    for _ in range(text_length):\n",
        "        subsequence = \"\".join(tokenizer.tokenize(\"\".join(word_sequence).lower())[current: current + n_words])\n",
        "\n",
        "        try:\n",
        "            choice = unique_tokens[random.choice(predict_next_word(subsequence, creativity))]\n",
        "        except:\n",
        "            choice = random.choice(unique_tokens)\n",
        "        word_tokens.append(choice)\n",
        "        current += 1\n",
        "\n",
        "    return \" \".join(word_tokens)\n",
        "\n",
        "# Generate text\n",
        "generated_text = generate_text(\"A precious gift for my siblings\", 100, 5)\n",
        "print(generated_text)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RxOwWTEr26v"
      },
      "outputs": [],
      "source": [
        "model = load_model(\"pre_trained_models/model.h5\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
